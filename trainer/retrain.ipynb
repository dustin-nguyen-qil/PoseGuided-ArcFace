{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from model.model import Backbone, MobileFaceNet, Am_softmax, Arcface, PoseArcFace\n",
    "import os.path as osp\n",
    "import os, shutil\n",
    "from utils.utils import separate_bn_paras\n",
    "from easydict import EasyDict as edict\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import pairwise_distances, roc_curve, auc\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrained_model = Backbone(50, drop_ratio=0.6, mode='ir_se')\n",
    "retrained_model.load_state_dict(torch.load('work_space/save/model_final_droneface.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data/photos_all_faces/'\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "groups = {}\n",
    "\n",
    "# Group the files by their first letter\n",
    "for file_name in files:\n",
    "    first_letter = file_name[0].upper()\n",
    "\n",
    "    if first_letter not in groups:\n",
    "        groups[first_letter] = []\n",
    "\n",
    "    groups[first_letter].append(file_name)\n",
    "\n",
    "# Create a new folder for each group and move the files\n",
    "for first_letter, files in groups.items():\n",
    "    new_folder_path = os.path.join(folder_path, first_letter)\n",
    "    os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "    for file_name in files:\n",
    "        old_file_path = os.path.join(folder_path, file_name)\n",
    "        new_file_path = os.path.join(new_folder_path, file_name)\n",
    "\n",
    "        shutil.move(old_file_path, new_file_path)\n",
    "\n",
    "    print(f\"Moved {len(files)} files to folder {first_letter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(training=True):\n",
    "    conf = edict()\n",
    "    conf.data_path = Path('data/photos_all_faces/')\n",
    "    conf.work_path = Path('work_space/')\n",
    "    conf.model_path = conf.work_path/'last_chechpoints'\n",
    "    conf.save_path = conf.work_path/'save'\n",
    "    conf.input_size = [112, 112]\n",
    "    conf.embedding_size = 512\n",
    "    conf.use_mobilefacenet = False\n",
    "    conf.net_depth = 50\n",
    "    conf.drop_ratio = 0.6\n",
    "    conf.net_mode = 'ir_se'\n",
    "    conf.class_num = 8\n",
    "    conf.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    conf.test_transform = transforms.Compose([\n",
    "        transforms.Resize((112, 112)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    conf.batch_size = 32\n",
    "\n",
    "    if training:\n",
    "        conf.log_path = conf.work_path/'retrain'\n",
    "        conf.save_path = conf.work_path/'save'\n",
    "        conf.lr = 1e-3\n",
    "        conf.momentum = 0.9\n",
    "        conf.pin_memory = True\n",
    "        conf.num_workers = 3\n",
    "        conf.ce_loss = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        pass \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = get_config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model, head and optimizer then load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "if conf.use_mobilefacenet:\n",
    "    model = MobileFaceNet(embedding_size=conf.net_depth).to(device)\n",
    "else:\n",
    "    model = Backbone(num_layers=conf.net_depth, drop_ratio=conf.drop_ratio, mode=conf.net_mode).to(device)\n",
    "\n",
    "paras_only_bn, paras_wo_bn = separate_bn_paras(model)\n",
    "\n",
    "head = Arcface(embedding_size=conf.embedding_size, classnum=conf.class_num).to(device)\n",
    "\n",
    "if conf.use_mobilefacenet:\n",
    "    optimizer = optim.SGD([\n",
    "        {'params': paras_wo_bn[:-1], 'weight_decay': 4e-5},\n",
    "        {'params': [paras_wo_bn[-1]] + [head.kernel], 'weight_decay': 4e-4},\n",
    "        {'params': paras_only_bn}\n",
    "    ], lr=conf.lr, momentum=conf.momentum)\n",
    "else:\n",
    "    optimizer = optim.SGD([\n",
    "        {'params': paras_wo_bn + [head.kernel], 'weight_decay': 5e-4},\n",
    "        {'params': paras_only_bn}\n",
    "    ], lr=conf.lr, momentum=conf.momentum)\n",
    "\n",
    "# checkpoint_path = conf.model_path\n",
    "\n",
    "if conf.use_mobilefacenet:\n",
    "    model_path = 'work_space/last_checkpoints/model_mobilefacenet.pth'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    model_path = 'work_space/last_checkpoints/model_ir_se50.pth'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "# head_path = 'work_space/last_checkpoints/head_2023-03-08-07-50_accuracy:0.8991428571428571_step:1346430_None.pth'\n",
    "# head.load_state_dict(torch.load(head_path))\n",
    "# optimizer_path = 'work_space/last_checkpoints/optimizer_2023-03-08-07-50_accuracy:0.8991428571428571_step:1346430_None.pth'\n",
    "# optimizer.load_state_dict(torch.load(optimizer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = osp.join(conf.data_path, 'train')\n",
    "test_dir = osp.join(conf.data_path, 'test')\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "train_data = ImageFolder(\n",
    "    root=train_dir,\n",
    "    transform=train_transform,\n",
    "    target_transform=None\n",
    ")\n",
    "test_data = ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=conf.test_transform,\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, pin_memory=conf.pin_memory, num_workers=conf.num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False, pin_memory=conf.pin_memory, num_workers=conf.num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started\n",
      "0.07955816212666235\n",
      "Epoch 1 started\n",
      "0.0950562267227704\n",
      "Epoch 2 started\n",
      "0.08993127986561271\n",
      "Epoch 3 started\n",
      "0.004169204059788397\n",
      "Epoch 4 started\n",
      "0.04273069532224194\n",
      "Epoch 5 started\n",
      "0.010708099017723344\n",
      "Epoch 6 started\n",
      "0.05907386103390898\n",
      "Epoch 7 started\n",
      "0.0420092335612425\n",
      "Epoch 8 started\n",
      "0.004678101877857408\n",
      "Epoch 9 started\n",
      "0.011995586384655999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train(conf, epochs, train_loader, model, optimizer, device):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0. \n",
    "        print(f'Epoch {epoch} started')\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            thetas = head(embeddings, labels)\n",
    "            loss = conf.ce_loss(thetas, labels)\n",
    "            running_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "        print(running_loss/len(train_loader))\n",
    "    return model \n",
    "\n",
    "trained_model = train(conf, 10, train_loader, model, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps each class name to a class index\n",
    "class_to_idx = test_data.class_to_idx\n",
    "\n",
    "# Create a list of image paths and class indices\n",
    "image_paths = []\n",
    "class_indices = []\n",
    "for target_class in class_to_idx.keys():\n",
    "    target_class_path = os.path.join(test_dir, target_class)\n",
    "    for image_filename in os.listdir(target_class_path):\n",
    "        image_path = os.path.join(target_class_path, image_filename)\n",
    "        image_paths.append(image_path)\n",
    "        class_indices.append(class_to_idx[target_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of image embeddings\n",
    "embeddings = []\n",
    "trained_model.eval()\n",
    "with torch.inference_mode():\n",
    "    for inputs, _ in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = trained_model(inputs)\n",
    "        embeddings.append(outputs)\n",
    "\n",
    "embeddings = torch.cat(embeddings).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(embeddings, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0033230094205703962"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the CMC curve\n",
    "# matches = (np.argsort(distances, axis=1) == class_indices[:, np.newaxis]).astype(np.int32)\n",
    "matches = (np.argsort(distances, axis=1) == np.array(class_indices)[:, np.newaxis]).astype(np.int32)\n",
    "cmc = np.cumsum(np.any(matches, axis=1))\n",
    "\n",
    "# Calculate the false positive rate and true positive rate for the ROC curve\n",
    "n_classes = len(np.unique(class_indices))\n",
    "n_imgs_per_class = len(class_indices) // n_classes\n",
    "true_labels = np.zeros(len(embeddings))\n",
    "for i, class_idx in enumerate(class_indices):\n",
    "    true_labels[class_idx: class_idx + n_imgs_per_class] = 1\n",
    "fpr, tpr, _ = roc_curve(true_labels, -embeddings[:, 0])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the CMC curve\n",
    "plt.plot(cmc / len(class_indices))\n",
    "plt.title('CMC curve')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Identification rate')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frald",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33a7ef05d333325ce5c4f11dc690ab48ad3dba3694a8255d230bea587f43ef76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
